{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SAE-embedding For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to SAE-embedding"},{"location":"#welcome-to-sae-embedding","text":"For full documentation visit mkdocs.org .","title":"Welcome to SAE-embedding"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"conclusion_technos/","text":"Architecture retenue Notre architecture front-end se d\u00e9coupe en deux niveaux : Astro : g\u00e8re le rendu statique, le routage, la structure globale et le SEO. Solid (Islands) : prend en charge les zones interactives cibl\u00e9es au sein des pages Astro. Cette approche hybride nous permet de b\u00e9n\u00e9ficier du meilleur des deux mondes : - la vitesse d\u2019un site statique , - et la souplesse d\u2019un framework r\u00e9actif l\u00e0 o\u00f9 c\u2019est utile.","title":"Conclusion"},{"location":"conclusion_technos/#architecture-retenue","text":"Notre architecture front-end se d\u00e9coupe en deux niveaux : Astro : g\u00e8re le rendu statique, le routage, la structure globale et le SEO. Solid (Islands) : prend en charge les zones interactives cibl\u00e9es au sein des pages Astro. Cette approche hybride nous permet de b\u00e9n\u00e9ficier du meilleur des deux mondes : - la vitesse d\u2019un site statique , - et la souplesse d\u2019un framework r\u00e9actif l\u00e0 o\u00f9 c\u2019est utile.","title":"Architecture retenue"},{"location":"contexte_technos/","text":"Notre d\u00e9marche Dans le cadre du d\u00e9veloppement du front-end de l\u2019application, notre \u00e9quipe a pris le temps d\u2019\u00e9valuer plusieurs frameworks modernes (React, Vue, Svelte, Solid, Astro, Next.js, etc.). L\u2019objectif \u00e9tait de trouver une solution qui allie performance, simplicit\u00e9, \u00e9volutivit\u00e9 et interactivit\u00e9 , tout en restant adapt\u00e9e au contexte de notre projet : une application l\u00e9g\u00e8re, rapide et facile \u00e0 maintenir . Apr\u00e8s comparaison, nous avons d\u00e9cid\u00e9 d\u2019adopter Astro comme framework principal, accompagn\u00e9 de SolidJS pour g\u00e9rer les parties interactives via le syst\u00e8me des Astro Islands .","title":"Introduction"},{"location":"contexte_technos/#notre-demarche","text":"Dans le cadre du d\u00e9veloppement du front-end de l\u2019application, notre \u00e9quipe a pris le temps d\u2019\u00e9valuer plusieurs frameworks modernes (React, Vue, Svelte, Solid, Astro, Next.js, etc.). L\u2019objectif \u00e9tait de trouver une solution qui allie performance, simplicit\u00e9, \u00e9volutivit\u00e9 et interactivit\u00e9 , tout en restant adapt\u00e9e au contexte de notre projet : une application l\u00e9g\u00e8re, rapide et facile \u00e0 maintenir . Apr\u00e8s comparaison, nous avons d\u00e9cid\u00e9 d\u2019adopter Astro comme framework principal, accompagn\u00e9 de SolidJS pour g\u00e9rer les parties interactives via le syst\u00e8me des Astro Islands .","title":"Notre d\u00e9marche"},{"location":"contextualized/","text":"Embeddings contextualis\u00e9s (Contextualized Word Embeddings) Ces embeddings tiennent compte du contexte dans lequel le mot appara\u00eet. Le m\u00eame mot peut donc avoir plusieurs repr\u00e9sentations diff\u00e9rentes selon la phrase (ex : \u201cbark\u201d en anglais peut d\u00e9signer un aboiement ou une \u00e9corce). Utilisent des architectures avanc\u00e9es comme les r\u00e9seaux LSTM bidirectionnels ou les Transformers. Exemples : ELMo : produit un vecteur pour chaque mot en tenant compte de toute la phrase, via des LSTM bidirectionnels. BERT : mod\u00e8le bas\u00e9 sur les Transformers ; produit des vecteurs contextuels riches pour chaque mot. GPT, RoBERTa, XLNet, etc. : d\u2019autres variantes de mod\u00e8les pr\u00e9entra\u00een\u00e9s de type Transformer. Avantages : tr\u00e8s performants, capturent le sens r\u00e9el du mot dans son contexte. Inconv\u00e9nients : lourds \u00e0 entra\u00eener et \u00e0 d\u00e9ployer, difficilement interpr\u00e9tables.","title":"Contextualized Word Embeddings"},{"location":"contextualized/#embeddings-contextualises-contextualized-word-embeddings","text":"Ces embeddings tiennent compte du contexte dans lequel le mot appara\u00eet. Le m\u00eame mot peut donc avoir plusieurs repr\u00e9sentations diff\u00e9rentes selon la phrase (ex : \u201cbark\u201d en anglais peut d\u00e9signer un aboiement ou une \u00e9corce). Utilisent des architectures avanc\u00e9es comme les r\u00e9seaux LSTM bidirectionnels ou les Transformers.","title":"Embeddings contextualis\u00e9s (Contextualized Word Embeddings)"},{"location":"contextualized/#exemples","text":"ELMo : produit un vecteur pour chaque mot en tenant compte de toute la phrase, via des LSTM bidirectionnels. BERT : mod\u00e8le bas\u00e9 sur les Transformers ; produit des vecteurs contextuels riches pour chaque mot. GPT, RoBERTa, XLNet, etc. : d\u2019autres variantes de mod\u00e8les pr\u00e9entra\u00een\u00e9s de type Transformer.","title":"Exemples :"},{"location":"contextualized/#avantages","text":"tr\u00e8s performants, capturent le sens r\u00e9el du mot dans son contexte.","title":"Avantages :"},{"location":"contextualized/#inconvenients","text":"lourds \u00e0 entra\u00eener et \u00e0 d\u00e9ployer, difficilement interpr\u00e9tables.","title":"Inconv\u00e9nients :"},{"location":"creation/","text":"Comment les embeddings sont cr\u00e9\u00e9s Les embeddings sont g\u00e9n\u00e9ralement appris automatiquement \u00e0 partir de grands corpus de texte, en utilisant des techniques d\u2019apprentissage automatique ou d\u2019apprentissage profond. L\u2019objectif principal est de trouver une repr\u00e9sentation num\u00e9rique pour chaque mot (ou phrase, ou document) qui capture ses relations avec les autres mots dans le langage. \u00c9tapes g\u00e9n\u00e9rales de cr\u00e9ation d\u2019un embedding 1. Pr\u00e9paration des donn\u00e9es : Un grand corpus de texte est collect\u00e9 (par exemple : Wikipedia, livres, articles...). Le texte est nettoy\u00e9, d\u00e9coup\u00e9 en phrases, puis en mots (tokenisation) Une \"fen\u00eatre de contexte\" est d\u00e9finie : par exemple, on peut regarder les 5 mots autour de chaque mot cible. 2. Construction d\u2019un objectif d\u2019apprentissage : On choisit un objectif \u00e0 optimiser. Par exemple : Pr\u00e9dire un mot cible \u00e0 partir de son contexte (CBOW). Pr\u00e9dire le contexte \u00e0 partir d\u2019un mot cible (Skip-gram). Ou bien apprendre une fonction qui approxime les relations de cooccurrence entre mots (GloVe). Ce sont des t\u00e2ches simples, mais qui forcent le mod\u00e8le \u00e0 comprendre les relations entre mots pour r\u00e9ussir. 3. Entra\u00eenement d\u2019un mod\u00e8le : On utilise un petit r\u00e9seau de neurones (dans Word2Vec) ou une architecture plus complexe (LSTM pour ELMo, Transformer pour BERT). Le mod\u00e8le ajuste progressivement les vecteurs associ\u00e9s \u00e0 chaque mot, de mani\u00e8re \u00e0 minimiser une fonction de perte (par exemple : erreur de pr\u00e9diction du mot). Pendant cet entra\u00eenement, chaque mot est associ\u00e9 \u00e0 un vecteur qui \u00e9volue \u00e0 chaque it\u00e9ration pour mieux refl\u00e9ter ses usages dans le corpus. 4. Extraction des vecteurs appris : Une fois l\u2019entra\u00eenement termin\u00e9, chaque mot a un vecteur num\u00e9rique stable : c\u2019est son embedding. Ces vecteurs peuvent ensuite \u00eatre utilis\u00e9s dans d\u2019autres mod\u00e8les pour diff\u00e9rentes t\u00e2ches (analyse de sentiments, traduction automatique, r\u00e9sum\u00e9 de texte, etc.).","title":"Cr\u00e9ation des embeddings"},{"location":"creation/#comment-les-embeddings-sont-crees","text":"Les embeddings sont g\u00e9n\u00e9ralement appris automatiquement \u00e0 partir de grands corpus de texte, en utilisant des techniques d\u2019apprentissage automatique ou d\u2019apprentissage profond. L\u2019objectif principal est de trouver une repr\u00e9sentation num\u00e9rique pour chaque mot (ou phrase, ou document) qui capture ses relations avec les autres mots dans le langage. \u00c9tapes g\u00e9n\u00e9rales de cr\u00e9ation d\u2019un embedding","title":"Comment les embeddings sont cr\u00e9\u00e9s"},{"location":"creation/#1-preparation-des-donnees","text":"Un grand corpus de texte est collect\u00e9 (par exemple : Wikipedia, livres, articles...). Le texte est nettoy\u00e9, d\u00e9coup\u00e9 en phrases, puis en mots (tokenisation) Une \"fen\u00eatre de contexte\" est d\u00e9finie : par exemple, on peut regarder les 5 mots autour de chaque mot cible.","title":"1. Pr\u00e9paration des donn\u00e9es :"},{"location":"creation/#2-construction-dun-objectif-dapprentissage","text":"On choisit un objectif \u00e0 optimiser. Par exemple : Pr\u00e9dire un mot cible \u00e0 partir de son contexte (CBOW). Pr\u00e9dire le contexte \u00e0 partir d\u2019un mot cible (Skip-gram). Ou bien apprendre une fonction qui approxime les relations de cooccurrence entre mots (GloVe). Ce sont des t\u00e2ches simples, mais qui forcent le mod\u00e8le \u00e0 comprendre les relations entre mots pour r\u00e9ussir.","title":"2. Construction d\u2019un objectif d\u2019apprentissage :"},{"location":"creation/#3-entrainement-dun-modele","text":"On utilise un petit r\u00e9seau de neurones (dans Word2Vec) ou une architecture plus complexe (LSTM pour ELMo, Transformer pour BERT). Le mod\u00e8le ajuste progressivement les vecteurs associ\u00e9s \u00e0 chaque mot, de mani\u00e8re \u00e0 minimiser une fonction de perte (par exemple : erreur de pr\u00e9diction du mot). Pendant cet entra\u00eenement, chaque mot est associ\u00e9 \u00e0 un vecteur qui \u00e9volue \u00e0 chaque it\u00e9ration pour mieux refl\u00e9ter ses usages dans le corpus.","title":"3. Entra\u00eenement d\u2019un mod\u00e8le :"},{"location":"creation/#4-extraction-des-vecteurs-appris","text":"Une fois l\u2019entra\u00eenement termin\u00e9, chaque mot a un vecteur num\u00e9rique stable : c\u2019est son embedding. Ces vecteurs peuvent ensuite \u00eatre utilis\u00e9s dans d\u2019autres mod\u00e8les pour diff\u00e9rentes t\u00e2ches (analyse de sentiments, traduction automatique, r\u00e9sum\u00e9 de texte, etc.).","title":"4. Extraction des vecteurs appris :"},{"location":"demarches/","text":"D\u00e9marches de test dans l'optique de tester les algorithmes d'embedding, nous avons mis en place un protocole de test, celui consiste \u00e0 \u00e9tablir un corpus de synonymes, d'antonymes, d'holonymes et de mots associ\u00e9s et d'ainsi calculer la proximit\u00e9 entre chacun des \u00e9l\u00e9ments par le mod\u00e8le ollama afin de calculer la moyenne de chaque \u00e9chantillon pour permettre de v\u00e9rifier l'\u00e9fficacit\u00e9 du mod\u00e8le. Th\u00e9oriquement, les synonymes devrait avoir le meilleur r\u00e9sultats conntrairement aux antonymes. \u00e9chantillion du corpus de synonymes \u00e0,chez,dans,parmi abaca,chanvre,ch\u00e8nevi\u00e8re,filasse,jute abaissable,abattable,inclinable abaissant,avilissant,humiliant,mortifiant,d\u00e9gradant,vexant,honteux,\u00e9crasant,blessant abaiss\u00e9,avili,rabaiss\u00e9,d\u00e9consid\u00e9r\u00e9,diminu\u00e9,d\u00e9chu,disqualifi\u00e9,discr\u00e9dit\u00e9,d\u00e9valoris\u00e9,d\u00e9valu\u00e9 abaisse-langue,spatule,manche","title":"D\u00e9marches"},{"location":"demarches/#demarches-de-test","text":"dans l'optique de tester les algorithmes d'embedding, nous avons mis en place un protocole de test, celui consiste \u00e0 \u00e9tablir un corpus de synonymes, d'antonymes, d'holonymes et de mots associ\u00e9s et d'ainsi calculer la proximit\u00e9 entre chacun des \u00e9l\u00e9ments par le mod\u00e8le ollama afin de calculer la moyenne de chaque \u00e9chantillon pour permettre de v\u00e9rifier l'\u00e9fficacit\u00e9 du mod\u00e8le. Th\u00e9oriquement, les synonymes devrait avoir le meilleur r\u00e9sultats conntrairement aux antonymes.","title":"D\u00e9marches de test"},{"location":"demarches/#echantillion-du-corpus-de-synonymes","text":"\u00e0,chez,dans,parmi abaca,chanvre,ch\u00e8nevi\u00e8re,filasse,jute abaissable,abattable,inclinable abaissant,avilissant,humiliant,mortifiant,d\u00e9gradant,vexant,honteux,\u00e9crasant,blessant abaiss\u00e9,avili,rabaiss\u00e9,d\u00e9consid\u00e9r\u00e9,diminu\u00e9,d\u00e9chu,disqualifi\u00e9,discr\u00e9dit\u00e9,d\u00e9valoris\u00e9,d\u00e9valu\u00e9 abaisse-langue,spatule,manche","title":"\u00e9chantillion du corpus de synonymes"},{"location":"frequency/","text":"Embeddings bas\u00e9s sur la fr\u00e9quence (Frequency-Based Embeddings) Ces m\u00e9thodes utilisent des statistiques de concurrence des mots dans un corpus. Elles reposent sur des matrices qui comptent le nombre de fois o\u00f9 des mots apparaissent ensemble dans des fen\u00eatres de texte. Exemples : Count Vector : vecteur bas\u00e9 sur le nombre d\u2019occurrences de chaque mot dans un document. TF-IDF (Term Frequency-Inverse Document Frequency) : pond\u00e8re les mots en fonction de leur fr\u00e9quence dans le document et dans le corpus. LSA (Latent Semantic Analysis) :utilise la d\u00e9composition SVD pour extraire des concepts latents \u00e0 partir d\u2019une matrice terme-document. Avantages : simples, interpr\u00e9tables. Inconv\u00e9nients : matrices tr\u00e8s grandes et creuses, peu efficaces pour capturer des liens s\u00e9mantiques complexes.","title":"Frequency-Based Embeddings"},{"location":"frequency/#embeddings-bases-sur-la-frequence-frequency-based-embeddings","text":"Ces m\u00e9thodes utilisent des statistiques de concurrence des mots dans un corpus. Elles reposent sur des matrices qui comptent le nombre de fois o\u00f9 des mots apparaissent ensemble dans des fen\u00eatres de texte.","title":"Embeddings bas\u00e9s sur la fr\u00e9quence (Frequency-Based Embeddings)"},{"location":"frequency/#exemples","text":"Count Vector : vecteur bas\u00e9 sur le nombre d\u2019occurrences de chaque mot dans un document. TF-IDF (Term Frequency-Inverse Document Frequency) : pond\u00e8re les mots en fonction de leur fr\u00e9quence dans le document et dans le corpus. LSA (Latent Semantic Analysis) :utilise la d\u00e9composition SVD pour extraire des concepts latents \u00e0 partir d\u2019une matrice terme-document.","title":"Exemples :"},{"location":"frequency/#avantages","text":"simples, interpr\u00e9tables.","title":"Avantages :"},{"location":"frequency/#inconvenients","text":"matrices tr\u00e8s grandes et creuses, peu efficaces pour capturer des liens s\u00e9mantiques complexes.","title":"Inconv\u00e9nients :"},{"location":"front-end_technos/","text":"Pourquoi nous avons choisi Astro ? Astro s\u2019est rapidement impos\u00e9 pour plusieurs raisons strat\u00e9giques : Crit\u00e8re Observation Impact sur le projet Performance initiale Rendu HTML statique, 0 JS par d\u00e9faut Temps de chargement ultra rapide Poids du bundle Tr\u00e8s faible (JS uniquement sur les composants interactifs) R\u00e9duction du co\u00fbt r\u00e9seau et meilleure UX Simplicit\u00e9 du code Structure claire, s\u00e9paration nette contenu / logique Maintenance facilit\u00e9e SEO et accessibilit\u00e9 Excellent gr\u00e2ce au pr\u00e9-rendu HTML Meilleur r\u00e9f\u00e9rencement naturel Facilit\u00e9 d\u2019int\u00e9gration Support hybride et partiel Int\u00e9gration fluide avec notre stack existante \u00c9cosyst\u00e8me en croissance Plugins officiels et communaut\u00e9 active Am\u00e9liorations et support \u00e0 long terme Nous avons retenu Astro car il permet de livrer un site ultra performant d\u00e8s le premier rendu , tout en restant suffisamment flexible pour int\u00e9grer des composants dynamiques grace \u00e0 astro islands. C\u2019est une approche \u201c content-first \u201d qui correspond parfaitement \u00e0 notre besoin : beaucoup de contenu statique, enrichi par quelques zones interactives.","title":"Front-end"},{"location":"front-end_technos/#pourquoi-nous-avons-choisi-astro","text":"Astro s\u2019est rapidement impos\u00e9 pour plusieurs raisons strat\u00e9giques : Crit\u00e8re Observation Impact sur le projet Performance initiale Rendu HTML statique, 0 JS par d\u00e9faut Temps de chargement ultra rapide Poids du bundle Tr\u00e8s faible (JS uniquement sur les composants interactifs) R\u00e9duction du co\u00fbt r\u00e9seau et meilleure UX Simplicit\u00e9 du code Structure claire, s\u00e9paration nette contenu / logique Maintenance facilit\u00e9e SEO et accessibilit\u00e9 Excellent gr\u00e2ce au pr\u00e9-rendu HTML Meilleur r\u00e9f\u00e9rencement naturel Facilit\u00e9 d\u2019int\u00e9gration Support hybride et partiel Int\u00e9gration fluide avec notre stack existante \u00c9cosyst\u00e8me en croissance Plugins officiels et communaut\u00e9 active Am\u00e9liorations et support \u00e0 long terme Nous avons retenu Astro car il permet de livrer un site ultra performant d\u00e8s le premier rendu , tout en restant suffisamment flexible pour int\u00e9grer des composants dynamiques grace \u00e0 astro islands. C\u2019est une approche \u201c content-first \u201d qui correspond parfaitement \u00e0 notre besoin : beaucoup de contenu statique, enrichi par quelques zones interactives.","title":"Pourquoi nous avons choisi Astro ?"},{"location":"integration-js_technos/","text":"Pourquoi nous avons Choisi Solid ? Certaines fonctionnalit\u00e9s de l\u2019application n\u00e9cessitent une interactivit\u00e9 locale due aux tableaux n\u00e9cessitant de pouvoir \u00eatre modifiables. Pour ces cas sp\u00e9cifiques, nous avons choisi SolidJS , int\u00e9gr\u00e9 via le syst\u00e8me des Astro Islands . Nos motivations principales : Crit\u00e8re Observation Avantage Performance runtime Solid est reconnu pour sa r\u00e9activit\u00e9 exceptionnelle Exp\u00e9rience fluide et instantan\u00e9e Poids du bundle Tr\u00e8s l\u00e9ger Interactivit\u00e9 sans compromettre la vitesse globale Simplicit\u00e9 d\u2019int\u00e9gration Compatible nativement avec Astro Aucun surco\u00fbt de configuration API moderne et famili\u00e8re Proche de React mais plus performante Courbe d\u2019apprentissage rapide pour l\u2019\u00e9quipe Solid n\u2019est charg\u00e9 que l\u00e0 o\u00f9 c\u2019est n\u00e9cessaire. Le reste du site reste statique et optimis\u00e9, ce qui nous permet de minimiser la quantit\u00e9 de JavaScript envoy\u00e9e au client , tout en maintenant une UX moderne et r\u00e9active.","title":"Int\u00e9gration JS"},{"location":"integration-js_technos/#pourquoi-nous-avons-choisi-solid","text":"Certaines fonctionnalit\u00e9s de l\u2019application n\u00e9cessitent une interactivit\u00e9 locale due aux tableaux n\u00e9cessitant de pouvoir \u00eatre modifiables. Pour ces cas sp\u00e9cifiques, nous avons choisi SolidJS , int\u00e9gr\u00e9 via le syst\u00e8me des Astro Islands .","title":"Pourquoi nous avons Choisi Solid ?"},{"location":"integration-js_technos/#nos-motivations-principales","text":"Crit\u00e8re Observation Avantage Performance runtime Solid est reconnu pour sa r\u00e9activit\u00e9 exceptionnelle Exp\u00e9rience fluide et instantan\u00e9e Poids du bundle Tr\u00e8s l\u00e9ger Interactivit\u00e9 sans compromettre la vitesse globale Simplicit\u00e9 d\u2019int\u00e9gration Compatible nativement avec Astro Aucun surco\u00fbt de configuration API moderne et famili\u00e8re Proche de React mais plus performante Courbe d\u2019apprentissage rapide pour l\u2019\u00e9quipe Solid n\u2019est charg\u00e9 que l\u00e0 o\u00f9 c\u2019est n\u00e9cessaire. Le reste du site reste statique et optimis\u00e9, ce qui nous permet de minimiser la quantit\u00e9 de JavaScript envoy\u00e9e au client , tout en maintenant une UX moderne et r\u00e9active.","title":"Nos motivations principales :"},{"location":"prediction/","text":"Embeddings pr\u00e9dictifs (Prediction-Based Embeddings) Ces mod\u00e8les apprennent les vecteurs en pr\u00e9voyant un mot \u00e0 partir de son contexte ou l\u2019inverse. Utilisent des r\u00e9seaux de neurones simples pour cr\u00e9er des repr\u00e9sentations distribu\u00e9es. Exemples : Word2Vec : Skip-gram : pr\u00e9dit les mots du contexte \u00e0 partir du mot cible. CBOW (Continuous Bag of Words) : pr\u00e9dit le mot cible \u00e0 partir du contexte. GloVe : combine approche de cooccurrence et apprentissage pr\u00e9dictif ; s\u2019appuie sur la probabilit\u00e9 relative de cooccurrence entre mots. Avantages : efficaces, rapides \u00e0 entra\u00eener, bonnes performances sur les similarit\u00e9s s\u00e9mantiques. Inconv\u00e9nients : un seul vecteur par mot, pas de prise en compte du contexte (un mot a toujours le m\u00eame vecteur, quel que soit son sens dans la phrase).","title":"Prediction-Based Embeddings"},{"location":"prediction/#embeddings-predictifs-prediction-based-embeddings","text":"Ces mod\u00e8les apprennent les vecteurs en pr\u00e9voyant un mot \u00e0 partir de son contexte ou l\u2019inverse. Utilisent des r\u00e9seaux de neurones simples pour cr\u00e9er des repr\u00e9sentations distribu\u00e9es.","title":"Embeddings pr\u00e9dictifs (Prediction-Based Embeddings)"},{"location":"prediction/#exemples","text":"","title":"Exemples :"},{"location":"prediction/#word2vec","text":"Skip-gram : pr\u00e9dit les mots du contexte \u00e0 partir du mot cible. CBOW (Continuous Bag of Words) : pr\u00e9dit le mot cible \u00e0 partir du contexte. GloVe : combine approche de cooccurrence et apprentissage pr\u00e9dictif ; s\u2019appuie sur la probabilit\u00e9 relative de cooccurrence entre mots.","title":"Word2Vec :"},{"location":"prediction/#avantages","text":"efficaces, rapides \u00e0 entra\u00eener, bonnes performances sur les similarit\u00e9s s\u00e9mantiques.","title":"Avantages :"},{"location":"prediction/#inconvenients","text":"un seul vecteur par mot, pas de prise en compte du contexte (un mot a toujours le m\u00eame vecteur, quel que soit son sens dans la phrase).","title":"Inconv\u00e9nients :"},{"location":"principes/","text":"Qu\u2019est-ce qu\u2019un embedding ? Un embedding est une repr\u00e9sentation num\u00e9rique (vecteur) d\u2019un mot, d\u2019une phrase ou d\u2019un document dans un espace de dimension r\u00e9duite. Contrairement aux repr\u00e9sentations traditionnelles comme le one-hot encoding, les embeddings sont des vecteurs denses qui contiennent de l\u2019information s\u00e9mantique. Chaque mot est repr\u00e9sent\u00e9 par un vecteur de taille fixe (par exemple, 100 ou 300 dimensions), dans lequel chaque dimension encode un aspect abstrait du mot. L\u2019id\u00e9e principale est que des mots ayant un sens similaire auront des vecteurs proches dans l\u2019espace vectoriel. Les embeddings sont souvent appris automatiquement \u00e0 partir de grands corpus de texte, sans supervision directe. Ils permettent aux algorithmes de traitement du langage naturel de comprendre les relations s\u00e9mantiques et syntaxiques entre les mots.","title":"Principes de l'embedding"},{"location":"principes/#quest-ce-quun-embedding","text":"Un embedding est une repr\u00e9sentation num\u00e9rique (vecteur) d\u2019un mot, d\u2019une phrase ou d\u2019un document dans un espace de dimension r\u00e9duite. Contrairement aux repr\u00e9sentations traditionnelles comme le one-hot encoding, les embeddings sont des vecteurs denses qui contiennent de l\u2019information s\u00e9mantique. Chaque mot est repr\u00e9sent\u00e9 par un vecteur de taille fixe (par exemple, 100 ou 300 dimensions), dans lequel chaque dimension encode un aspect abstrait du mot. L\u2019id\u00e9e principale est que des mots ayant un sens similaire auront des vecteurs proches dans l\u2019espace vectoriel. Les embeddings sont souvent appris automatiquement \u00e0 partir de grands corpus de texte, sans supervision directe. Ils permettent aux algorithmes de traitement du langage naturel de comprendre les relations s\u00e9mantiques et syntaxiques entre les mots.","title":"Qu\u2019est-ce qu\u2019un embedding ?"},{"location":"resultats/","text":"","title":"R\u00e9sultats"}]}