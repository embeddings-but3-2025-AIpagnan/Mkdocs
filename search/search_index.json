{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Qu\u2019est-ce qu\u2019un embedding ? Un embedding est une repr\u00e9sentation num\u00e9rique (vecteur) d\u2019un mot, d\u2019une phrase ou d\u2019un document dans un espace de dimension r\u00e9duite. Contrairement aux repr\u00e9sentations traditionnelles comme le one-hot encoding, les embeddings sont des vecteurs denses qui contiennent de l\u2019information s\u00e9mantique. Chaque mot est repr\u00e9sent\u00e9 par un vecteur de taille fixe (par exemple, 100 ou 300 dimensions), dans lequel chaque dimension encode un aspect abstrait du mot. L\u2019id\u00e9e principale est que des mots ayant un sens similaire auront des vecteurs proches dans l\u2019espace vectoriel. Les embeddings sont souvent appris automatiquement \u00e0 partir de grands corpus de texte, sans supervision directe. Ils permettent aux algorithmes de traitement du langage naturel de comprendre les relations s\u00e9mantiques et syntaxiques entre les mots.","title":"Principes de l'embedding"},{"location":"#quest-ce-quun-embedding","text":"Un embedding est une repr\u00e9sentation num\u00e9rique (vecteur) d\u2019un mot, d\u2019une phrase ou d\u2019un document dans un espace de dimension r\u00e9duite. Contrairement aux repr\u00e9sentations traditionnelles comme le one-hot encoding, les embeddings sont des vecteurs denses qui contiennent de l\u2019information s\u00e9mantique. Chaque mot est repr\u00e9sent\u00e9 par un vecteur de taille fixe (par exemple, 100 ou 300 dimensions), dans lequel chaque dimension encode un aspect abstrait du mot. L\u2019id\u00e9e principale est que des mots ayant un sens similaire auront des vecteurs proches dans l\u2019espace vectoriel. Les embeddings sont souvent appris automatiquement \u00e0 partir de grands corpus de texte, sans supervision directe. Ils permettent aux algorithmes de traitement du langage naturel de comprendre les relations s\u00e9mantiques et syntaxiques entre les mots.","title":"Qu\u2019est-ce qu\u2019un embedding ?"},{"location":"conclusion_technos/","text":"Architecture retenue Notre architecture front-end se d\u00e9coupe en deux niveaux : Astro : g\u00e8re le rendu statique, le routage, la structure globale et le SEO. Solid (Islands) : prend en charge les zones interactives cibl\u00e9es au sein des pages Astro. Cette approche hybride nous permet de b\u00e9n\u00e9ficier du meilleur des deux mondes : - la vitesse d\u2019un site statique , - et la souplesse d\u2019un framework r\u00e9actif l\u00e0 o\u00f9 c\u2019est utile.","title":"Conclusion"},{"location":"conclusion_technos/#architecture-retenue","text":"Notre architecture front-end se d\u00e9coupe en deux niveaux : Astro : g\u00e8re le rendu statique, le routage, la structure globale et le SEO. Solid (Islands) : prend en charge les zones interactives cibl\u00e9es au sein des pages Astro. Cette approche hybride nous permet de b\u00e9n\u00e9ficier du meilleur des deux mondes : - la vitesse d\u2019un site statique , - et la souplesse d\u2019un framework r\u00e9actif l\u00e0 o\u00f9 c\u2019est utile.","title":"Architecture retenue"},{"location":"contexte_technos/","text":"Notre d\u00e9marche Dans le cadre du d\u00e9veloppement du front-end de l\u2019application, notre \u00e9quipe a pris le temps d\u2019\u00e9valuer plusieurs frameworks modernes (React, Vue, Svelte, Solid, Astro, Next.js, etc.). L\u2019objectif \u00e9tait de trouver une solution qui allie performance, simplicit\u00e9, \u00e9volutivit\u00e9 et interactivit\u00e9 , tout en restant adapt\u00e9e au contexte de notre projet : une application l\u00e9g\u00e8re, rapide et facile \u00e0 maintenir . Apr\u00e8s comparaison, nous avons d\u00e9cid\u00e9 d\u2019adopter Astro comme framework principal, accompagn\u00e9 de SolidJS pour g\u00e9rer les parties interactives via le syst\u00e8me des Astro Islands .","title":"Introduction"},{"location":"contexte_technos/#notre-demarche","text":"Dans le cadre du d\u00e9veloppement du front-end de l\u2019application, notre \u00e9quipe a pris le temps d\u2019\u00e9valuer plusieurs frameworks modernes (React, Vue, Svelte, Solid, Astro, Next.js, etc.). L\u2019objectif \u00e9tait de trouver une solution qui allie performance, simplicit\u00e9, \u00e9volutivit\u00e9 et interactivit\u00e9 , tout en restant adapt\u00e9e au contexte de notre projet : une application l\u00e9g\u00e8re, rapide et facile \u00e0 maintenir . Apr\u00e8s comparaison, nous avons d\u00e9cid\u00e9 d\u2019adopter Astro comme framework principal, accompagn\u00e9 de SolidJS pour g\u00e9rer les parties interactives via le syst\u00e8me des Astro Islands .","title":"Notre d\u00e9marche"},{"location":"contextualized/","text":"Embeddings contextualis\u00e9s (Contextualized Word Embeddings) Ces embeddings tiennent compte du contexte dans lequel le mot appara\u00eet. Le m\u00eame mot peut donc avoir plusieurs repr\u00e9sentations diff\u00e9rentes selon la phrase (ex : \u201cbark\u201d en anglais peut d\u00e9signer un aboiement ou une \u00e9corce). Utilisent des architectures avanc\u00e9es comme les r\u00e9seaux LSTM bidirectionnels ou les Transformers. Exemples ELMo : produit un vecteur pour chaque mot en tenant compte de toute la phrase, via des LSTM bidirectionnels. BERT : mod\u00e8le bas\u00e9 sur les Transformers ; produit des vecteurs contextuels riches pour chaque mot. GPT, RoBERTa, XLNet, etc. : d\u2019autres variantes de mod\u00e8les pr\u00e9entra\u00een\u00e9s de type Transformer. Avantages tr\u00e8s performants, capturent le sens r\u00e9el du mot dans son contexte. Inconv\u00e9nients lourds \u00e0 entra\u00eener et \u00e0 d\u00e9ployer, difficilement interpr\u00e9tables.","title":"Contextualized Word Embeddings"},{"location":"contextualized/#embeddings-contextualises-contextualized-word-embeddings","text":"Ces embeddings tiennent compte du contexte dans lequel le mot appara\u00eet. Le m\u00eame mot peut donc avoir plusieurs repr\u00e9sentations diff\u00e9rentes selon la phrase (ex : \u201cbark\u201d en anglais peut d\u00e9signer un aboiement ou une \u00e9corce). Utilisent des architectures avanc\u00e9es comme les r\u00e9seaux LSTM bidirectionnels ou les Transformers.","title":"Embeddings contextualis\u00e9s (Contextualized Word Embeddings)"},{"location":"contextualized/#exemples","text":"ELMo : produit un vecteur pour chaque mot en tenant compte de toute la phrase, via des LSTM bidirectionnels. BERT : mod\u00e8le bas\u00e9 sur les Transformers ; produit des vecteurs contextuels riches pour chaque mot. GPT, RoBERTa, XLNet, etc. : d\u2019autres variantes de mod\u00e8les pr\u00e9entra\u00een\u00e9s de type Transformer.","title":"Exemples"},{"location":"contextualized/#avantages","text":"tr\u00e8s performants, capturent le sens r\u00e9el du mot dans son contexte.","title":"Avantages"},{"location":"contextualized/#inconvenients","text":"lourds \u00e0 entra\u00eener et \u00e0 d\u00e9ployer, difficilement interpr\u00e9tables.","title":"Inconv\u00e9nients"},{"location":"creation/","text":"Comment les embeddings sont cr\u00e9\u00e9s Les embeddings sont g\u00e9n\u00e9ralement appris automatiquement \u00e0 partir de grands corpus de texte, en utilisant des techniques d\u2019apprentissage automatique ou d\u2019apprentissage profond. L\u2019objectif principal est de trouver une repr\u00e9sentation num\u00e9rique pour chaque mot (ou phrase, ou document) qui capture ses relations avec les autres mots dans le langage. \u00c9tapes g\u00e9n\u00e9rales de cr\u00e9ation d\u2019un embedding 1. Pr\u00e9paration des donn\u00e9es Un grand corpus de texte est collect\u00e9 (par exemple : Wikipedia, livres, articles...). Le texte est nettoy\u00e9, d\u00e9coup\u00e9 en phrases, puis en mots (tokenisation). Une \"fen\u00eatre de contexte\" est d\u00e9finie : par exemple, on peut regarder les 5 mots autour de chaque mot cible. 2. Construction d\u2019un objectif d\u2019apprentissage On choisit un objectif \u00e0 optimiser. Par exemple : Pr\u00e9dire un mot cible \u00e0 partir de son contexte (CBOW). Pr\u00e9dire le contexte \u00e0 partir d\u2019un mot cible (Skip-gram). Ou bien apprendre une fonction qui approxime les relations de cooccurrence entre mots (GloVe). Ce sont des t\u00e2ches simples, mais qui forcent le mod\u00e8le \u00e0 comprendre les relations entre mots pour r\u00e9ussir. 3. Entra\u00eenement d\u2019un mod\u00e8le On utilise un petit r\u00e9seau de neurones (dans Word2Vec) ou une architecture plus complexe (LSTM pour ELMo, Transformer pour BERT). Le mod\u00e8le ajuste progressivement les vecteurs associ\u00e9s \u00e0 chaque mot, de mani\u00e8re \u00e0 minimiser une fonction de perte (par exemple : erreur de pr\u00e9diction du mot). Pendant cet entra\u00eenement, chaque mot est associ\u00e9 \u00e0 un vecteur qui \u00e9volue \u00e0 chaque it\u00e9ration pour mieux refl\u00e9ter ses usages dans le corpus. 4. Extraction des vecteurs appris Une fois l\u2019entra\u00eenement termin\u00e9, chaque mot a un vecteur num\u00e9rique stable : c\u2019est son embedding. Ces vecteurs peuvent ensuite \u00eatre utilis\u00e9s dans d\u2019autres mod\u00e8les pour diff\u00e9rentes t\u00e2ches (analyse de sentiments, traduction automatique, r\u00e9sum\u00e9 de texte, etc.).","title":"Cr\u00e9ation des embeddings"},{"location":"creation/#comment-les-embeddings-sont-crees","text":"Les embeddings sont g\u00e9n\u00e9ralement appris automatiquement \u00e0 partir de grands corpus de texte, en utilisant des techniques d\u2019apprentissage automatique ou d\u2019apprentissage profond. L\u2019objectif principal est de trouver une repr\u00e9sentation num\u00e9rique pour chaque mot (ou phrase, ou document) qui capture ses relations avec les autres mots dans le langage. \u00c9tapes g\u00e9n\u00e9rales de cr\u00e9ation d\u2019un embedding","title":"Comment les embeddings sont cr\u00e9\u00e9s"},{"location":"creation/#1-preparation-des-donnees","text":"Un grand corpus de texte est collect\u00e9 (par exemple : Wikipedia, livres, articles...). Le texte est nettoy\u00e9, d\u00e9coup\u00e9 en phrases, puis en mots (tokenisation). Une \"fen\u00eatre de contexte\" est d\u00e9finie : par exemple, on peut regarder les 5 mots autour de chaque mot cible.","title":"1. Pr\u00e9paration des donn\u00e9es"},{"location":"creation/#2-construction-dun-objectif-dapprentissage","text":"On choisit un objectif \u00e0 optimiser. Par exemple : Pr\u00e9dire un mot cible \u00e0 partir de son contexte (CBOW). Pr\u00e9dire le contexte \u00e0 partir d\u2019un mot cible (Skip-gram). Ou bien apprendre une fonction qui approxime les relations de cooccurrence entre mots (GloVe). Ce sont des t\u00e2ches simples, mais qui forcent le mod\u00e8le \u00e0 comprendre les relations entre mots pour r\u00e9ussir.","title":"2. Construction d\u2019un objectif d\u2019apprentissage"},{"location":"creation/#3-entrainement-dun-modele","text":"On utilise un petit r\u00e9seau de neurones (dans Word2Vec) ou une architecture plus complexe (LSTM pour ELMo, Transformer pour BERT). Le mod\u00e8le ajuste progressivement les vecteurs associ\u00e9s \u00e0 chaque mot, de mani\u00e8re \u00e0 minimiser une fonction de perte (par exemple : erreur de pr\u00e9diction du mot). Pendant cet entra\u00eenement, chaque mot est associ\u00e9 \u00e0 un vecteur qui \u00e9volue \u00e0 chaque it\u00e9ration pour mieux refl\u00e9ter ses usages dans le corpus.","title":"3. Entra\u00eenement d\u2019un mod\u00e8le"},{"location":"creation/#4-extraction-des-vecteurs-appris","text":"Une fois l\u2019entra\u00eenement termin\u00e9, chaque mot a un vecteur num\u00e9rique stable : c\u2019est son embedding. Ces vecteurs peuvent ensuite \u00eatre utilis\u00e9s dans d\u2019autres mod\u00e8les pour diff\u00e9rentes t\u00e2ches (analyse de sentiments, traduction automatique, r\u00e9sum\u00e9 de texte, etc.).","title":"4. Extraction des vecteurs appris"},{"location":"deploiment/","text":"","title":"d\u00e9ploiment"},{"location":"ebauches/","text":"","title":"Premi\u00e8res \u00e9bauches"},{"location":"frequency/","text":"Embeddings bas\u00e9s sur la fr\u00e9quence (Frequency-Based Embeddings) Ces m\u00e9thodes utilisent des statistiques de concurrence des mots dans un corpus. Elles reposent sur des matrices qui comptent le nombre de fois o\u00f9 des mots apparaissent ensemble dans des fen\u00eatres de texte. Exemples Count Vector : vecteur bas\u00e9 sur le nombre d\u2019occurrences de chaque mot dans un document. TF-IDF (Term Frequency-Inverse Document Frequency) : pond\u00e8re les mots en fonction de leur fr\u00e9quence dans le document et dans le corpus. LSA (Latent Semantic Analysis) :utilise la d\u00e9composition SVD pour extraire des concepts latents \u00e0 partir d\u2019une matrice terme-document. Avantages simples, interpr\u00e9tables. Inconv\u00e9nients matrices tr\u00e8s grandes et creuses, peu efficaces pour capturer des liens s\u00e9mantiques complexes.","title":"Frequency-Based Embeddings"},{"location":"frequency/#embeddings-bases-sur-la-frequence-frequency-based-embeddings","text":"Ces m\u00e9thodes utilisent des statistiques de concurrence des mots dans un corpus. Elles reposent sur des matrices qui comptent le nombre de fois o\u00f9 des mots apparaissent ensemble dans des fen\u00eatres de texte.","title":"Embeddings bas\u00e9s sur la fr\u00e9quence (Frequency-Based Embeddings)"},{"location":"frequency/#exemples","text":"Count Vector : vecteur bas\u00e9 sur le nombre d\u2019occurrences de chaque mot dans un document. TF-IDF (Term Frequency-Inverse Document Frequency) : pond\u00e8re les mots en fonction de leur fr\u00e9quence dans le document et dans le corpus. LSA (Latent Semantic Analysis) :utilise la d\u00e9composition SVD pour extraire des concepts latents \u00e0 partir d\u2019une matrice terme-document.","title":"Exemples"},{"location":"frequency/#avantages","text":"simples, interpr\u00e9tables.","title":"Avantages"},{"location":"frequency/#inconvenients","text":"matrices tr\u00e8s grandes et creuses, peu efficaces pour capturer des liens s\u00e9mantiques complexes.","title":"Inconv\u00e9nients"},{"location":"front-end_technos/","text":"Pourquoi nous avons choisi Astro ? Astro s\u2019est rapidement impos\u00e9 pour plusieurs raisons strat\u00e9giques : Crit\u00e8re Observation Impact sur le projet Performance initiale Rendu HTML statique, 0 JS par d\u00e9faut Temps de chargement ultra rapide Poids du bundle Tr\u00e8s faible (JS uniquement sur les composants interactifs) R\u00e9duction du co\u00fbt r\u00e9seau et meilleure UX Simplicit\u00e9 du code Structure claire, s\u00e9paration nette contenu / logique Maintenance facilit\u00e9e SEO et accessibilit\u00e9 Excellent gr\u00e2ce au pr\u00e9-rendu HTML Meilleur r\u00e9f\u00e9rencement naturel Facilit\u00e9 d\u2019int\u00e9gration Support hybride et partiel Int\u00e9gration fluide avec notre stack existante \u00c9cosyst\u00e8me en croissance Plugins officiels et communaut\u00e9 active Am\u00e9liorations et support \u00e0 long terme Nous avons retenu Astro car il permet de livrer un site ultra performant d\u00e8s le premier rendu , tout en restant suffisamment flexible pour int\u00e9grer des composants dynamiques grace \u00e0 astro islands. C\u2019est une approche \u201c content-first \u201d, c'est \u00e0 dire priorisant l'affichage de donn\u00e9es, qui correspond parfaitement \u00e0 notre besoin : beaucoup de contenu statique, enrichi par quelques zones interactives.","title":"Front-end"},{"location":"front-end_technos/#pourquoi-nous-avons-choisi-astro","text":"Astro s\u2019est rapidement impos\u00e9 pour plusieurs raisons strat\u00e9giques : Crit\u00e8re Observation Impact sur le projet Performance initiale Rendu HTML statique, 0 JS par d\u00e9faut Temps de chargement ultra rapide Poids du bundle Tr\u00e8s faible (JS uniquement sur les composants interactifs) R\u00e9duction du co\u00fbt r\u00e9seau et meilleure UX Simplicit\u00e9 du code Structure claire, s\u00e9paration nette contenu / logique Maintenance facilit\u00e9e SEO et accessibilit\u00e9 Excellent gr\u00e2ce au pr\u00e9-rendu HTML Meilleur r\u00e9f\u00e9rencement naturel Facilit\u00e9 d\u2019int\u00e9gration Support hybride et partiel Int\u00e9gration fluide avec notre stack existante \u00c9cosyst\u00e8me en croissance Plugins officiels et communaut\u00e9 active Am\u00e9liorations et support \u00e0 long terme Nous avons retenu Astro car il permet de livrer un site ultra performant d\u00e8s le premier rendu , tout en restant suffisamment flexible pour int\u00e9grer des composants dynamiques grace \u00e0 astro islands. C\u2019est une approche \u201c content-first \u201d, c'est \u00e0 dire priorisant l'affichage de donn\u00e9es, qui correspond parfaitement \u00e0 notre besoin : beaucoup de contenu statique, enrichi par quelques zones interactives.","title":"Pourquoi nous avons choisi Astro ?"},{"location":"inspiration/","text":"Inspiration pour les maquettes Pour trouver de l'inspiration pour les maquettes, nous avons demand\u00e9 \u00e0 diff\u00e9rentes ia de nous sugg\u00e9rer des maquettes, nous allons donc comparer, les diff\u00e9rents r\u00e9sultats, \u00e9tablir leurs forces et faiblesses afin de cr\u00e9er une maquette finales. Le prompt que nous avons utilis\u00e9 est le suivant: Propose-moi un design pour une application web qui me permette de cr\u00e9er un glossaire m\u00e9tier. Je voudrais pouvoir ajouter un mot, sa d\u00e9finition, ainsi qu'un contexte li\u00e9 \u00e0 ce mot. Il faudrait aussi que je puisse entrer des synonymes et des antonymes, et que l'IA puisse en sugg\u00e9rer . Je voudrais avoir la possibilit\u00e9 de choisir les synonymes les plus pertinents. De plus, en fonction des mots d\u00e9j\u00e0 enregistr\u00e9s, l\u2019application devrait pouvoir proposer des contextes associ\u00e9s. deepseek La maquette propos\u00e9e par Deepseek est correcte mais trop charg\u00e9e. chatgpt ChatGPT a propos\u00e9 une maquette assez similaire \u00e0 celle de Deepseek mais avec comme principale diff\u00e9rence la pr\u00e9sentation des diff\u00e9rents termes qui se font sous forme d'un tableur, ce qui nous int\u00e9resse plus. claude Claude a g\u00e9n\u00e9r\u00e9 une maquette trop simpliste et vide avec des couleurs \u00e9tranges. gemini Gemini a propos\u00e9 un desgin r\u00e9pondant certaines de nos exigeances, nous nous inspirons de plusieurs \u00e9l\u00e9ments de ce design. grok Le design de Grok r\u00e9pond quant \u00e0 lui presque parfaitement \u00e0 nos besoins \u00e0 l'exception de la pr\u00e9sentation des termes existants, il nous servira d'inspiration principale. mistral La pr\u00e9sentation de mistral est pl\u00fbtot archa\u00efque. qwen Qwen nous a propos\u00e9 un design assez vide et simple, insuffisant pour en tirer de l'inspiration. perplexity perplexity nous pr\u00e9sente une interface avec les m\u00eames d\u00e9faut que celle de qwen. stitch . Stitch \u00e9tant une intelligence artificielle sp\u00e9cialis\u00e9e en design, elle nous propose plusieurs maquettes fournies assez semblables aux designs modernes mais est assez impersonnelles, cependant cela servira d'inspiration principale pour le tableur.","title":"Inspiration"},{"location":"inspiration/#inspiration-pour-les-maquettes","text":"Pour trouver de l'inspiration pour les maquettes, nous avons demand\u00e9 \u00e0 diff\u00e9rentes ia de nous sugg\u00e9rer des maquettes, nous allons donc comparer, les diff\u00e9rents r\u00e9sultats, \u00e9tablir leurs forces et faiblesses afin de cr\u00e9er une maquette finales. Le prompt que nous avons utilis\u00e9 est le suivant: Propose-moi un design pour une application web qui me permette de cr\u00e9er un glossaire m\u00e9tier. Je voudrais pouvoir ajouter un mot, sa d\u00e9finition, ainsi qu'un contexte li\u00e9 \u00e0 ce mot. Il faudrait aussi que je puisse entrer des synonymes et des antonymes, et que l'IA puisse en sugg\u00e9rer . Je voudrais avoir la possibilit\u00e9 de choisir les synonymes les plus pertinents. De plus, en fonction des mots d\u00e9j\u00e0 enregistr\u00e9s, l\u2019application devrait pouvoir proposer des contextes associ\u00e9s.","title":"Inspiration pour les maquettes"},{"location":"inspiration/#deepseek","text":"La maquette propos\u00e9e par Deepseek est correcte mais trop charg\u00e9e.","title":"deepseek"},{"location":"inspiration/#chatgpt","text":"ChatGPT a propos\u00e9 une maquette assez similaire \u00e0 celle de Deepseek mais avec comme principale diff\u00e9rence la pr\u00e9sentation des diff\u00e9rents termes qui se font sous forme d'un tableur, ce qui nous int\u00e9resse plus.","title":"chatgpt"},{"location":"inspiration/#claude","text":"Claude a g\u00e9n\u00e9r\u00e9 une maquette trop simpliste et vide avec des couleurs \u00e9tranges.","title":"claude"},{"location":"inspiration/#gemini","text":"Gemini a propos\u00e9 un desgin r\u00e9pondant certaines de nos exigeances, nous nous inspirons de plusieurs \u00e9l\u00e9ments de ce design.","title":"gemini"},{"location":"inspiration/#grok","text":"Le design de Grok r\u00e9pond quant \u00e0 lui presque parfaitement \u00e0 nos besoins \u00e0 l'exception de la pr\u00e9sentation des termes existants, il nous servira d'inspiration principale.","title":"grok"},{"location":"inspiration/#mistral","text":"La pr\u00e9sentation de mistral est pl\u00fbtot archa\u00efque.","title":"mistral"},{"location":"inspiration/#qwen","text":"Qwen nous a propos\u00e9 un design assez vide et simple, insuffisant pour en tirer de l'inspiration.","title":"qwen"},{"location":"inspiration/#perplexity","text":"perplexity nous pr\u00e9sente une interface avec les m\u00eames d\u00e9faut que celle de qwen.","title":"perplexity"},{"location":"inspiration/#stitch","text":". Stitch \u00e9tant une intelligence artificielle sp\u00e9cialis\u00e9e en design, elle nous propose plusieurs maquettes fournies assez semblables aux designs modernes mais est assez impersonnelles, cependant cela servira d'inspiration principale pour le tableur.","title":"stitch"},{"location":"installation/","text":"Installation Linux Afin d'installer l'application sur les syst\u00e8mes d'exploitation linux, il suffit de double cliquer sur la appImage pr\u00e9sent sur le r\u00e9pertoire.","title":"installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#linux","text":"Afin d'installer l'application sur les syst\u00e8mes d'exploitation linux, il suffit de double cliquer sur la appImage pr\u00e9sent sur le r\u00e9pertoire.","title":"Linux"},{"location":"integration-js_technos/","text":"Pourquoi nous avons Choisi Solid ? Certaines fonctionnalit\u00e9s de l\u2019application n\u00e9cessitent une interactivit\u00e9 locale due aux tableaux n\u00e9cessitant de pouvoir \u00eatre modifiables. Pour ces cas sp\u00e9cifiques, nous avons choisi SolidJS , int\u00e9gr\u00e9 via le syst\u00e8me des Astro Islands . Nos motivations principales Crit\u00e8re Observation Avantage Performance runtime Solid est reconnu pour sa r\u00e9activit\u00e9 exceptionnelle Exp\u00e9rience fluide et instantan\u00e9e Poids du bundle Tr\u00e8s l\u00e9ger Interactivit\u00e9 sans compromettre la vitesse globale Simplicit\u00e9 d\u2019int\u00e9gration Compatible nativement avec Astro Aucun surco\u00fbt de configuration API moderne et famili\u00e8re Proche de React mais plus performante Courbe d\u2019apprentissage rapide pour l\u2019\u00e9quipe Solid n\u2019est charg\u00e9 que l\u00e0 o\u00f9 c\u2019est n\u00e9cessaire. Le reste du site reste statique et optimis\u00e9, ce qui nous permet de minimiser la quantit\u00e9 de JavaScript envoy\u00e9e au client , tout en maintenant une UX moderne et r\u00e9active.","title":"Int\u00e9gration JS"},{"location":"integration-js_technos/#pourquoi-nous-avons-choisi-solid","text":"Certaines fonctionnalit\u00e9s de l\u2019application n\u00e9cessitent une interactivit\u00e9 locale due aux tableaux n\u00e9cessitant de pouvoir \u00eatre modifiables. Pour ces cas sp\u00e9cifiques, nous avons choisi SolidJS , int\u00e9gr\u00e9 via le syst\u00e8me des Astro Islands .","title":"Pourquoi nous avons Choisi Solid ?"},{"location":"integration-js_technos/#nos-motivations-principales","text":"Crit\u00e8re Observation Avantage Performance runtime Solid est reconnu pour sa r\u00e9activit\u00e9 exceptionnelle Exp\u00e9rience fluide et instantan\u00e9e Poids du bundle Tr\u00e8s l\u00e9ger Interactivit\u00e9 sans compromettre la vitesse globale Simplicit\u00e9 d\u2019int\u00e9gration Compatible nativement avec Astro Aucun surco\u00fbt de configuration API moderne et famili\u00e8re Proche de React mais plus performante Courbe d\u2019apprentissage rapide pour l\u2019\u00e9quipe Solid n\u2019est charg\u00e9 que l\u00e0 o\u00f9 c\u2019est n\u00e9cessaire. Le reste du site reste statique et optimis\u00e9, ce qui nous permet de minimiser la quantit\u00e9 de JavaScript envoy\u00e9e au client , tout en maintenant une UX moderne et r\u00e9active.","title":"Nos motivations principales"},{"location":"prediction/","text":"Embeddings pr\u00e9dictifs (Prediction-Based Embeddings) Ces mod\u00e8les apprennent les vecteurs en pr\u00e9voyant un mot \u00e0 partir de son contexte ou l\u2019inverse. Utilisent des r\u00e9seaux de neurones simples pour cr\u00e9er des repr\u00e9sentations distribu\u00e9es. Exemples Word2Vec Skip-gram : pr\u00e9dit les mots du contexte \u00e0 partir du mot cible. CBOW (Continuous Bag of Words) : pr\u00e9dit le mot cible \u00e0 partir du contexte. GloVe : combine approche de cooccurrence et apprentissage pr\u00e9dictif ; s\u2019appuie sur la probabilit\u00e9 relative de cooccurrence entre mots. Avantages efficaces, rapides \u00e0 entra\u00eener, bonnes performances sur les similarit\u00e9s s\u00e9mantiques. Inconv\u00e9nients un seul vecteur par mot, pas de prise en compte du contexte (un mot a toujours le m\u00eame vecteur, quel que soit son sens dans la phrase).","title":"Prediction-Based Embeddings"},{"location":"prediction/#embeddings-predictifs-prediction-based-embeddings","text":"Ces mod\u00e8les apprennent les vecteurs en pr\u00e9voyant un mot \u00e0 partir de son contexte ou l\u2019inverse. Utilisent des r\u00e9seaux de neurones simples pour cr\u00e9er des repr\u00e9sentations distribu\u00e9es.","title":"Embeddings pr\u00e9dictifs (Prediction-Based Embeddings)"},{"location":"prediction/#exemples","text":"","title":"Exemples"},{"location":"prediction/#word2vec","text":"Skip-gram : pr\u00e9dit les mots du contexte \u00e0 partir du mot cible. CBOW (Continuous Bag of Words) : pr\u00e9dit le mot cible \u00e0 partir du contexte. GloVe : combine approche de cooccurrence et apprentissage pr\u00e9dictif ; s\u2019appuie sur la probabilit\u00e9 relative de cooccurrence entre mots.","title":"Word2Vec"},{"location":"prediction/#avantages","text":"efficaces, rapides \u00e0 entra\u00eener, bonnes performances sur les similarit\u00e9s s\u00e9mantiques.","title":"Avantages"},{"location":"prediction/#inconvenients","text":"un seul vecteur par mot, pas de prise en compte du contexte (un mot a toujours le m\u00eame vecteur, quel que soit son sens dans la phrase).","title":"Inconv\u00e9nients"},{"location":"schema/","text":"Sch\u00e9ma d'architecture","title":"sch\u00e9ma"},{"location":"schema/#schema-darchitecture","text":"","title":"Sch\u00e9ma d'architecture"},{"location":"experimentations/api_visualisations/","text":"","title":"API & Visualisations"},{"location":"experimentations/premieres_experiences/","text":"Choix des technologies Pour commencer, il nous a fallu choisir une technologie \u00e0 utiliser. Comme nous n'arrivions pas \u00e0 faire fonctionner Word2Vec (pour des raisons techniques), nous avons initialement choisi d'utiliser un mod\u00e8le bas\u00e9 sur des transformers, facile \u00e0 lancer n'importe o\u00f9 gr\u00e2ce \u00e0 Ollama. Nous avons ensuite r\u00e9ussi \u00e0 faire fonctionner Word2Vec, et nous avons donc initialement d\u00e9cid\u00e9 de comparer les deux. Pour Word2Vec, nous avons utilis\u00e9 le corpus text8, qui contient le premier milliard de caract\u00e8res de Wikipedia. L'autre mod\u00e8le que nous avons utilis\u00e9 est MiniLM, qui est bas\u00e9 sur une architecture BERT (qui est elle-m\u00eame bas\u00e9e sur une architecture de transformers). Tests s\u00e9mantiques sur MiniLM Nous avons commenc\u00e9 par faire des tests pour voir s'il \u00e9tait possible de se baser sur la proximit\u00e9 (distance scalaire) pour trouver des relations entre les mots. Nous avons donc calcul\u00e9 la distance moyenne entre des groupes de synonymes, antonymes, mots associ\u00e9s et holonymes (relation de tout \u00e0 partie). Pour les synonymes, nous avons trouv\u00e9 un corpus de 36 000 groupes de synonymes. Nous n'avons pas trouv\u00e9 de corpus pour les autres types de mots, nous les avons donc g\u00e9n\u00e9r\u00e9s par IA. Voici les r\u00e9sultats: Synonymes Proximit\u00e9 moyenne: 0.637 Proximit\u00e9 minimale: 0.484 pour \u00e9pistaxis, saignement de nez Proximit\u00e9 maximale: 0.976 pour scintigramme, scintillogramme Antonymes Proximit\u00e9 moyenne: 0.665 Proximit\u00e9 minimale: 0.565 pour aube, cr\u00e9puscule Proximit\u00e9 maximale: 0.907 pour visible, invisible Associ\u00e9s Proximit\u00e9 moyenne: 0.640 Proximit\u00e9 minimale: 0.554 pour lion, crini\u00e8re Proximit\u00e9 maximale: 0.794 pour pluie, parapluie Holonymes Proximit\u00e9 moyenne: 0.653 Proximit\u00e9 minimale: 0.537 pour organe, corps Proximit\u00e9 maximale: 0.854 pour graine, fruit \u00c0 notre grande d\u00e9ception, tous les types de mots ont une proximit\u00e9 similaire. Apr\u00e8s r\u00e9flexion, cela para\u00eet logique, car m\u00eame si la relation entre les mots est diff\u00e9rente, ils restent proches s\u00e9mantiquement. R\u00e9cup\u00e9ration de synonymes via Word2Vec Contrairement \u00e0 MiniLM, Word2Vec permet de r\u00e9cup\u00e9rer la liste des mots qui sont proches d'un vecteur. Nous avons donc test\u00e9 de trouver des synonymes avec la m\u00e9thode suivante : on part d'un (ou plusieurs) mot(s), on les embed (c'est-\u00e0-dire les convertit en vecteur), on fait la moyenne des vecteurs obtenus, puis on r\u00e9cup\u00e8re la liste des mots les plus proches de ce vecteur. Voici les r\u00e9sultats (la proximit\u00e9 au vecteur est entre parenth\u00e8ses): Avec les mots: home, residence, house house ( 0.802 ) residence ( 0.788 ) home ( 0.749 ) palace ( 0.693 ) mansion ( 0.678 ) manor ( 0.661 ) castle ( 0.631 ) windsor ( 0.628 ) edinburgh ( 0.625 ) hotel ( 0.620 ) Avec les mots: button, zipper, latch button ( 0.989 ) buttons ( 0.829 ) hook ( 0.781 ) stick ( 0.769 ) punch ( 0.763 ) notch ( 0.762 ) pedal ( 0.754 ) tray ( 0.751 ) window ( 0.749 ) keyboard ( 0.748 ) Avec les mots: penalty, sanction, sentence penalty ( 0.861 ) sentence ( 0.849 ) defendant ( 0.712 ) felony ( 0.683 ) manslaughter ( 0.669 ) procedure ( 0.666 ) punishment ( 0.652 ) offence ( 0.634 ) conviction ( 0.634 ) wrongful ( 0.628 ) Avec les mots: ruby, emerald, sapphire sapphire ( 0.856 ) ruby ( 0.855 ) microcebus ( 0.836 ) plum ( 0.829 ) moth ( 0.825 ) bean ( 0.823 ) ginger ( 0.820 ) cabbage ( 0.819 ) emerald ( 0.817 ) marmoset ( 0.805 ) Comme on peut le voir, la qualit\u00e9 des mots est plut\u00f4t mauvaise, et les scores ne permettent pas de filtrer les meilleurs mots. En plus de cela, Word2Vec ne supporte pas le contexte (c'est-\u00e0-dire qu'on ne peut embedder qu'un seul mot \u00e0 la fois). M\u00eame si nous avons simul\u00e9 du contexte en faisant la moyenne de plusieurs mots, c'est tout de m\u00eame moins qualitatif qu'un mod\u00e8le qui permet d'utiliser du contexte. Au vu du manque d'efficacit\u00e9 de cette m\u00e9thode, nous avons d\u00e9cid\u00e9 de changer de m\u00e9thode pour d\u00e9terminer les synonymes d'un mot.","title":"Premi\u00e8res exp\u00e9riences"},{"location":"experimentations/premieres_experiences/#choix-des-technologies","text":"Pour commencer, il nous a fallu choisir une technologie \u00e0 utiliser. Comme nous n'arrivions pas \u00e0 faire fonctionner Word2Vec (pour des raisons techniques), nous avons initialement choisi d'utiliser un mod\u00e8le bas\u00e9 sur des transformers, facile \u00e0 lancer n'importe o\u00f9 gr\u00e2ce \u00e0 Ollama. Nous avons ensuite r\u00e9ussi \u00e0 faire fonctionner Word2Vec, et nous avons donc initialement d\u00e9cid\u00e9 de comparer les deux. Pour Word2Vec, nous avons utilis\u00e9 le corpus text8, qui contient le premier milliard de caract\u00e8res de Wikipedia. L'autre mod\u00e8le que nous avons utilis\u00e9 est MiniLM, qui est bas\u00e9 sur une architecture BERT (qui est elle-m\u00eame bas\u00e9e sur une architecture de transformers).","title":"Choix des technologies"},{"location":"experimentations/premieres_experiences/#tests-semantiques-sur-minilm","text":"Nous avons commenc\u00e9 par faire des tests pour voir s'il \u00e9tait possible de se baser sur la proximit\u00e9 (distance scalaire) pour trouver des relations entre les mots. Nous avons donc calcul\u00e9 la distance moyenne entre des groupes de synonymes, antonymes, mots associ\u00e9s et holonymes (relation de tout \u00e0 partie). Pour les synonymes, nous avons trouv\u00e9 un corpus de 36 000 groupes de synonymes. Nous n'avons pas trouv\u00e9 de corpus pour les autres types de mots, nous les avons donc g\u00e9n\u00e9r\u00e9s par IA. Voici les r\u00e9sultats: Synonymes Proximit\u00e9 moyenne: 0.637 Proximit\u00e9 minimale: 0.484 pour \u00e9pistaxis, saignement de nez Proximit\u00e9 maximale: 0.976 pour scintigramme, scintillogramme Antonymes Proximit\u00e9 moyenne: 0.665 Proximit\u00e9 minimale: 0.565 pour aube, cr\u00e9puscule Proximit\u00e9 maximale: 0.907 pour visible, invisible Associ\u00e9s Proximit\u00e9 moyenne: 0.640 Proximit\u00e9 minimale: 0.554 pour lion, crini\u00e8re Proximit\u00e9 maximale: 0.794 pour pluie, parapluie Holonymes Proximit\u00e9 moyenne: 0.653 Proximit\u00e9 minimale: 0.537 pour organe, corps Proximit\u00e9 maximale: 0.854 pour graine, fruit \u00c0 notre grande d\u00e9ception, tous les types de mots ont une proximit\u00e9 similaire. Apr\u00e8s r\u00e9flexion, cela para\u00eet logique, car m\u00eame si la relation entre les mots est diff\u00e9rente, ils restent proches s\u00e9mantiquement.","title":"Tests s\u00e9mantiques sur MiniLM"},{"location":"experimentations/premieres_experiences/#recuperation-de-synonymes-via-word2vec","text":"Contrairement \u00e0 MiniLM, Word2Vec permet de r\u00e9cup\u00e9rer la liste des mots qui sont proches d'un vecteur. Nous avons donc test\u00e9 de trouver des synonymes avec la m\u00e9thode suivante : on part d'un (ou plusieurs) mot(s), on les embed (c'est-\u00e0-dire les convertit en vecteur), on fait la moyenne des vecteurs obtenus, puis on r\u00e9cup\u00e8re la liste des mots les plus proches de ce vecteur. Voici les r\u00e9sultats (la proximit\u00e9 au vecteur est entre parenth\u00e8ses): Avec les mots: home, residence, house house ( 0.802 ) residence ( 0.788 ) home ( 0.749 ) palace ( 0.693 ) mansion ( 0.678 ) manor ( 0.661 ) castle ( 0.631 ) windsor ( 0.628 ) edinburgh ( 0.625 ) hotel ( 0.620 ) Avec les mots: button, zipper, latch button ( 0.989 ) buttons ( 0.829 ) hook ( 0.781 ) stick ( 0.769 ) punch ( 0.763 ) notch ( 0.762 ) pedal ( 0.754 ) tray ( 0.751 ) window ( 0.749 ) keyboard ( 0.748 ) Avec les mots: penalty, sanction, sentence penalty ( 0.861 ) sentence ( 0.849 ) defendant ( 0.712 ) felony ( 0.683 ) manslaughter ( 0.669 ) procedure ( 0.666 ) punishment ( 0.652 ) offence ( 0.634 ) conviction ( 0.634 ) wrongful ( 0.628 ) Avec les mots: ruby, emerald, sapphire sapphire ( 0.856 ) ruby ( 0.855 ) microcebus ( 0.836 ) plum ( 0.829 ) moth ( 0.825 ) bean ( 0.823 ) ginger ( 0.820 ) cabbage ( 0.819 ) emerald ( 0.817 ) marmoset ( 0.805 ) Comme on peut le voir, la qualit\u00e9 des mots est plut\u00f4t mauvaise, et les scores ne permettent pas de filtrer les meilleurs mots. En plus de cela, Word2Vec ne supporte pas le contexte (c'est-\u00e0-dire qu'on ne peut embedder qu'un seul mot \u00e0 la fois). M\u00eame si nous avons simul\u00e9 du contexte en faisant la moyenne de plusieurs mots, c'est tout de m\u00eame moins qualitatif qu'un mod\u00e8le qui permet d'utiliser du contexte. Au vu du manque d'efficacit\u00e9 de cette m\u00e9thode, nous avons d\u00e9cid\u00e9 de changer de m\u00e9thode pour d\u00e9terminer les synonymes d'un mot.","title":"R\u00e9cup\u00e9ration de synonymes via Word2Vec"}]}